# CrossMAEæ”¹é€ è®¡åˆ’ - æ°´æ–‡å¤šæ¨¡æ€MAE

## ğŸ“‹ æ€»ä½“ç›®æ ‡
å°†åŸºç¡€MAEçš„Self-Attention Decoderæ”¹é€ ä¸ºCrossMAEé£æ ¼çš„Cross-Attention Decoderï¼Œ
æ”¯æŒWeightedFeatureMapsï¼ˆå¯é€‰å¯ç”¨ï¼‰

## ğŸ¯ æ ¸å¿ƒæ”¹åŠ¨æ¦‚è§ˆ

### éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
1. âœ… `models/layers.py` - æ·»åŠ CrossAttentionå’ŒCrossAttentionBlock
2. âœ… `models/image_encoder.py` - æ”¯æŒè¾“å‡ºå¤šå±‚features
3. âœ… `models/image_decoder.py` - æ”¹é€ ä¸ºCrossAttention decoder
4. âœ… `models/vector_encoder.py` - æ”¯æŒè¾“å‡ºå¤šå±‚features
5. âœ… `models/vector_decoder.py` - æ”¹é€ ä¸ºCrossAttention decoder
6. âœ… `configs/mae_config.py` - æ·»åŠ CrossMAEç›¸å…³é…ç½®
7. âœ… `models/multimodal_mae.py` - é€‚é…æ–°çš„encoder/decoderæ¥å£

---

## ğŸ“ è¯¦ç»†æ”¹åŠ¨è®¡åˆ’

### Step 1: æ·»åŠ CrossAttentionæ¨¡å—
**æ–‡ä»¶**: `models/layers.py`

**éœ€è¦æ·»åŠ **:
```python
class CrossAttention(nn.Module):
    """
    Cross-Attention: queries from decoder, keys/values from encoder

    å‚è€ƒ: CrossMAE transformer_utils.py:69-108
    """
    def __init__(self, encoder_dim, decoder_dim, num_heads=8, ...):
        # Query projection (from decoder)
        self.q = nn.Linear(decoder_dim, decoder_dim, bias=qkv_bias)
        # Key-Value projection (from encoder)
        self.kv = nn.Linear(encoder_dim, decoder_dim * 2, bias=qkv_bias)
        # ...

    def forward(self, x, y):
        """
        x: [B, N_masked, decoder_dim] - decoder queries
        y: [B, N_visible, encoder_dim] - encoder keys/values
        """
        # Q from x, K/V from y
        q = self.q(x)
        kv = self.kv(y)
        k, v = split kv

        # Cross-attention
        attn = (q @ k.T) * scale
        out = attn @ v
        return out

class CrossAttentionBlock(nn.Module):
    """
    Transformer block with cross-attention

    å‚è€ƒ: CrossMAE transformer_utils.py:129-156
    """
    def __init__(self, encoder_dim, decoder_dim, num_heads,
                 self_attn=False, ...):
        # Optional self-attention (masked tokensä¹‹é—´)
        if self_attn:
            self.self_attn = Attention(decoder_dim, ...)

        # Cross-attention (masked query visible)
        self.cross_attn = CrossAttention(encoder_dim, decoder_dim, ...)

        # FFN
        self.mlp = Mlp(...)

    def forward(self, x, y):
        # Optional: masked self-attention
        if self.self_attn:
            x = x + self.self_attn(norm(x))

        # Cross-attention
        x = x + self.cross_attn(norm(x), y)

        # FFN
        x = x + self.mlp(norm(x))
        return x

class WeightedFeatureMaps(nn.Module):
    """
    å­¦ä¹ å¦‚ä½•ç»„åˆå¤šå±‚encoder features

    å‚è€ƒ: CrossMAE models_cross.py:23-40
    """
    def __init__(self, num_layers, embed_dim, decoder_depth):
        # ä¸ºæ¯ä¸ªdecoderå±‚å­¦ä¹ encoderå±‚çš„æƒé‡
        self.linear = nn.Linear(num_layers, decoder_depth, bias=False)
        # åˆå§‹åŒ–æƒé‡
        std = 1. / math.sqrt(num_layers)
        nn.init.normal_(self.linear.weight, mean=0., std=std)

    def forward(self, feature_maps):
        """
        feature_maps: list of [B, L, C] tensors
        Returns: [B, L, C, decoder_depth]
        """
        stacked = torch.stack(feature_maps, dim=-1)  # [B, L, C, k]
        output = self.linear(stacked)  # [B, L, C, decoder_depth]
        return output
```

---

### Step 2: æ”¹é€ Image Encoder
**æ–‡ä»¶**: `models/image_encoder.py`

**å…³é”®æ”¹åŠ¨**:

1. **æ·»åŠ é…ç½®å‚æ•°** (åœ¨`__init__`):
```python
def __init__(self,
             ...,
             use_weighted_fm=False,  # æ˜¯å¦ä½¿ç”¨WeightedFeatureMaps
             use_fm_layers=None,     # ä½¿ç”¨å“ªäº›å±‚ [0, 2, 4, 5] or None (all)
             use_input=False):       # æ˜¯å¦åŒ…å«è¾“å…¥ä½œä¸ºç¬¬0å±‚

    self.use_weighted_fm = use_weighted_fm
    self.use_input = use_input

    # å†³å®šä½¿ç”¨å“ªäº›å±‚
    if use_fm_layers is None:
        self.use_fm_layers = list(range(num_layers))  # æ‰€æœ‰å±‚
    else:
        self.use_fm_layers = use_fm_layers
```

2. **ä¿®æ”¹forwardå‡½æ•°** (å‚è€ƒ CrossMAE models_cross.py:205-230):
```python
def forward(self, x_img, patch_mask):
    """
    Returns:
        encoder_token: [B, d_model] or list of features
        mask_info: dict
    """
    B, T, H, W = x_img.shape

    # Patchify and filter valid patches
    patches = patchify(x_img, self.patch_size)
    patches = patches[:, :, self.valid_patch_indices, :]
    patch_mask_valid = patch_mask[:, :, self.valid_patch_indices]

    # Remove masked patches
    visible_patches_list = []
    visible_positions_list = []
    lengths = []

    for b in range(B):
        sample_patches = []
        sample_positions = []
        for t in range(T):
            visible_mask_t = ~patch_mask_valid[b, t]
            visible_patches_t = patches[b, t, visible_mask_t]
            sample_patches.append(visible_patches_t)
            # ... record positions
        visible_patches_list.append(torch.cat(sample_patches))
        lengths.append(len(sample_patches))

    # Pad to max_len
    max_len = max(lengths)
    x_padded = torch.zeros(B, max_len, self.patch_dim, device=x_img.device)
    for b in range(B):
        x_padded[b, :lengths[b]] = visible_patches_list[b]

    padding_mask = torch.zeros(B, max_len, device=x_img.device, dtype=torch.bool)
    for b in range(B):
        if lengths[b] < max_len:
            padding_mask[b, lengths[b]:] = True

    # Patch embedding
    x = self.patch_embed(x_padded)

    # Add position embeddings
    for b in range(B):
        for i, (t_idx, patch_idx) in enumerate(positions_padded[b]):
            x[b, i] += self.spatial_pos[0, patch_idx]
            x[b, i] += self.temporal_pos.pe[0, t_idx]

    # ===== æ–°å¢ï¼šæ”¶é›†å¤šå±‚features =====
    if self.use_weighted_fm:
        x_feats = []

        # å¯é€‰ï¼šæ·»åŠ è¾“å…¥ä½œä¸ºç¬¬0å±‚
        if self.use_input:
            x_feats.append(x.clone())

        # Transformer blocks
        for idx, blk in enumerate(self.transformer.layers):
            x = blk(x, src_key_padding_mask=padding_mask)

            # ä¿å­˜æŒ‡å®šå±‚çš„è¾“å‡º
            if idx in self.use_fm_layers:
                x_feats.append(x.clone())

        # Poolingæ¯ä¸€å±‚
        encoder_tokens = []
        valid_mask = (~padding_mask).unsqueeze(-1).float()
        for feat in x_feats:
            token = (feat * valid_mask).sum(dim=1) / valid_mask.sum(dim=1)
            encoder_tokens.append(self.norm(token))

        # è¿”å›list of tokens
        mask_info = {
            'mask': patch_mask,
            'lengths': lengths,
            'padding_mask': padding_mask,
            'positions': positions_padded,
        }

        return encoder_tokens, mask_info  # list of [B, d_model]

    else:
        # æ ‡å‡†MAEï¼šåªè¿”å›æœ€åä¸€å±‚
        x = self.transformer(x, src_key_padding_mask=padding_mask)

        # Pooling
        valid_mask = (~padding_mask).unsqueeze(-1).float()
        encoder_token = (x * valid_mask).sum(dim=1) / valid_mask.sum(dim=1)
        encoder_token = self.norm(encoder_token)

        mask_info = {
            'mask': patch_mask,
            'lengths': lengths,
        }

        return encoder_token, mask_info  # [B, d_model]
```

---

### Step 3: æ”¹é€ Image Decoder
**æ–‡ä»¶**: `models/image_decoder.py`

**é‡å¤§æ”¹åŠ¨** (å‚è€ƒ CrossMAE models_cross.py:240-256):

1. **ä¿®æ”¹åˆå§‹åŒ–**:
```python
def __init__(self,
             encoder_dim: int = 256,
             decoder_dim: int = 128,
             num_patches: int = 522,
             patch_dim: int = 100,
             num_decoder_layers: int = 4,
             nhead: int = 8,
             max_time_steps: int = 90,
             dropout: float = 0.1,
             use_weighted_fm: bool = False,      # æ–°å¢
             num_encoder_layers: int = 6,        # æ–°å¢
             use_cross_attn: bool = True,        # æ–°å¢ï¼šæ˜¯å¦ç”¨CrossAttention
             self_attn: bool = False):           # æ–°å¢ï¼šæ˜¯å¦ç”¨masked self-attn

    super().__init__()

    self.encoder_dim = encoder_dim
    self.decoder_dim = decoder_dim
    self.use_weighted_fm = use_weighted_fm
    self.use_cross_attn = use_cross_attn

    # Project encoder token to decoder dimension (å¦‚æœä¸ç”¨cross-attn)
    if not use_cross_attn:
        self.encoder_to_decoder = nn.Linear(encoder_dim, decoder_dim)

    # Mask token
    self.mask_token = nn.Parameter(torch.zeros(1, 1, 1, decoder_dim))
    nn.init.normal_(self.mask_token, std=0.02)

    # WeightedFeatureMaps (å¯é€‰)
    if use_weighted_fm:
        # ä¸ºæ¯ä¸ªdecoderå±‚å‡†å¤‡norm
        self.dec_norms = nn.ModuleList([
            nn.LayerNorm(encoder_dim)
            for _ in range(num_decoder_layers)
        ])

        # Feature weighting module
        self.wfm = WeightedFeatureMaps(
            num_layers=num_encoder_layers,
            embed_dim=encoder_dim,
            decoder_depth=num_decoder_layers
        )

    # Decoder blocks
    if use_cross_attn:
        # CrossAttention blocks
        self.decoder_blocks = nn.ModuleList([
            CrossAttentionBlock(
                encoder_dim=encoder_dim,
                decoder_dim=decoder_dim,
                num_heads=nhead,
                self_attn=self_attn,  # å¯é€‰çš„masked self-attn
                dropout=dropout
            )
            for _ in range(num_decoder_layers)
        ])
    else:
        # æ ‡å‡†Self-Attention blocks (fallback)
        decoder_layer = nn.TransformerEncoderLayer(...)
        self.transformer = nn.TransformerEncoder(decoder_layer, num_decoder_layers)

    # Spatial positional embedding
    self.spatial_pos = nn.Parameter(torch.zeros(1, 1, num_patches, decoder_dim))
    nn.init.normal_(self.spatial_pos, std=0.02)

    # Temporal positional encoding
    self.temporal_pos = PositionalEncoding(decoder_dim, max_time_steps)

    # Prediction head
    self.pred_head = nn.Linear(decoder_dim, patch_dim)
    self.decoder_norm = nn.LayerNorm(decoder_dim)
```

2. **å…³é”®ï¼šä¿®æ”¹forwardå‡½æ•°**:
```python
def forward(self, encoder_output, mask_info):
    """
    Args:
        encoder_output: [B, encoder_dim] or list of [B, encoder_dim]
        mask_info: dict with 'mask', 'lengths', etc.

    Returns:
        pred_patches: [B, T, num_patches, patch_dim]
    """
    mask = mask_info['mask']  # [B, T, num_patches]
    B, T, num_patches = mask.shape

    if self.use_cross_attn:
        # ===== CrossMAEé£æ ¼ =====
        return self._forward_cross_attn(encoder_output, mask_info)
    else:
        # ===== æ ‡å‡†MAEé£æ ¼ (fallback) =====
        return self._forward_self_attn(encoder_output, mask_info)

def _forward_cross_attn(self, encoder_output, mask_info):
    """CrossAttentionç‰ˆæœ¬çš„decoder"""
    mask = mask_info['mask']
    B, T, num_patches = mask.shape

    # Step 1: åªåˆ›å»ºmasked positionsçš„queries
    # å‚è€ƒ CrossMAE models_cross.py:232-238
    masked_queries = []
    masked_positions = []

    for b in range(B):
        for t in range(T):
            masked_indices = torch.where(mask[b, t])[0]  # True = masked
            num_masked_t = len(masked_indices)

            if num_masked_t > 0:
                # ä¸ºæ¯ä¸ªmasked positionåˆ›å»ºquery
                for patch_idx in masked_indices:
                    # mask_token + positional embedding
                    query = self.mask_token.squeeze() + \
                            self.spatial_pos[0, 0, patch_idx] + \
                            self.temporal_pos.pe[0, t]
                    masked_queries.append(query)
                    masked_positions.append((b, t, patch_idx.item()))

    if len(masked_queries) == 0:
        # Edge case: no masked patches
        return torch.zeros(B, T, num_patches, self.patch_dim, device=mask.device)

    # Stack all masked queries
    queries = torch.stack(masked_queries, dim=0)  # [total_masked, decoder_dim]
    queries = queries.unsqueeze(0)  # [1, total_masked, decoder_dim]

    # Step 2: Prepare encoder outputs as keys/values
    if self.use_weighted_fm:
        # encoder_outputæ˜¯list of [B, encoder_dim]
        # éœ€è¦ç»„åˆæˆ [B, encoder_dim, decoder_depth]
        encoder_feats = self.wfm(encoder_output)  # [B, encoder_dim, decoder_depth]
    else:
        # encoder_outputæ˜¯å•ä¸ª [B, encoder_dim]
        encoder_feats = encoder_output.unsqueeze(1)  # [B, 1, encoder_dim]

    # Step 3: CrossAttention decoder
    x = queries

    if self.use_weighted_fm:
        # æ¯ä¸ªdecoderå±‚ç”¨ä¸åŒçš„encoder featureç»„åˆ
        for i, blk in enumerate(self.decoder_blocks):
            # è·å–ç¬¬iä¸ªdecoderå±‚å¯¹åº”çš„encoder features
            y = self.dec_norms[i](encoder_feats[..., i])  # [B, encoder_dim]
            y = y.unsqueeze(1).expand(-1, x.shape[1], -1)  # [B, total_masked, encoder_dim]

            x = blk(x, y)  # CrossAttentionBlock
    else:
        # æ‰€æœ‰decoderå±‚ç”¨ç›¸åŒçš„encoder output
        y = encoder_feats.expand(-1, x.shape[1], -1)  # [B, total_masked, encoder_dim]

        for blk in self.decoder_blocks:
            x = blk(x, y)

    # Step 4: Prediction
    x = self.decoder_norm(x)
    predictions = self.pred_head(x)  # [1, total_masked, patch_dim]

    # Step 5: é‡ç»„å› [B, T, num_patches, patch_dim]
    pred_patches = torch.zeros(B, T, num_patches, self.patch_dim,
                               device=mask.device, dtype=predictions.dtype)

    for idx, (b, t, patch_idx) in enumerate(masked_positions):
        pred_patches[b, t, patch_idx] = predictions[0, idx]

    return pred_patches

def _forward_self_attn(self, encoder_token, mask_info):
    """æ ‡å‡†Self-Attentionç‰ˆæœ¬ (fallback)"""
    # è¿™æ˜¯ä½ ç°æœ‰çš„ä»£ç é€»è¾‘
    # ... ä¿æŒä¸å˜
    pass
```

---

### Step 4: Vector Encoderæ”¹é€ 
**æ–‡ä»¶**: `models/vector_encoder.py`

**æ”¹åŠ¨**: ä¸Image Encoderç±»ä¼¼ï¼Œæ”¯æŒè¾“å‡ºå¤šå±‚features

```python
def __init__(self, ...,
             use_weighted_fm=False,
             use_fm_layers=None,
             use_input=False):
    # ... åŒImage Encoder

def forward(self, x_vec, vec_mask, static_attr):
    # ... å¤„ç†visible vectors

    if self.use_weighted_fm:
        x_feats = []

        if self.use_input:
            x_feats.append(x.clone())

        for idx, blk in enumerate(self.transformer.layers):
            x = blk(x, src_key_padding_mask=padding_mask)

            if idx in self.use_fm_layers:
                x_feats.append(x.clone())

        # Pooling each layer
        encoder_tokens = []
        for feat in x_feats:
            token = ... # pooling
            encoder_tokens.append(token)

        return encoder_tokens, mask_info  # list
    else:
        # æ ‡å‡†ç‰ˆæœ¬
        x = self.transformer(x, src_key_padding_mask=padding_mask)
        encoder_token = ... # pooling
        return encoder_token, mask_info
```

---

### Step 5: Vector Decoderæ”¹é€ 
**æ–‡ä»¶**: `models/vector_decoder.py`

**æ”¹åŠ¨**: ä¸Image Decoderç±»ä¼¼

```python
def __init__(self, ...,
             use_weighted_fm=False,
             num_encoder_layers=4,
             use_cross_attn=True,
             self_attn=False):
    # ... åŒImage Decoder

    if use_cross_attn:
        self.decoder_blocks = nn.ModuleList([
            CrossAttentionBlock(encoder_dim, decoder_dim, ...)
            for _ in range(num_decoder_layers)
        ])

    if use_weighted_fm:
        self.wfm = WeightedFeatureMaps(...)
        self.dec_norms = nn.ModuleList([...])

def forward(self, encoder_output, mask_info):
    if self.use_cross_attn:
        return self._forward_cross_attn(encoder_output, mask_info)
    else:
        return self._forward_self_attn(encoder_output, mask_info)
```

---

### Step 6: é…ç½®æ–‡ä»¶
**æ–‡ä»¶**: `configs/mae_config.py`

**æ·»åŠ æ–°é…ç½®**:
```python
class MAEConfig:
    # ... ç°æœ‰é…ç½®

    # ========== CrossMAE Configuration ==========
    # Decoder type
    use_cross_attn = True  # Use CrossAttention instead of Self-Attention

    # Weighted Feature Maps
    use_weighted_fm = False  # Enable WeightedFeatureMaps (å¤šå±‚encoder features)
    use_fm_layers = None  # Which encoder layers to use [0, 2, 4, 5] or None (all)
    use_input = False  # Include input as layer 0

    # Optional masked self-attention in decoder
    decoder_self_attn = False  # Add self-attention in decoder (default: False)
```

---

### Step 7: ä¸»æ¨¡å‹é€‚é…
**æ–‡ä»¶**: `models/multimodal_mae.py`

**ä¿®æ”¹forwardé€»è¾‘**:
```python
def forward(self, batch):
    # ... å‰ç½®å¤„ç†

    # Encoder
    precip_output, precip_mask_info = self.precip_encoder(
        batch['precip'], batch['precip_mask']
    )
    # precip_outputå¯èƒ½æ˜¯ [B, D] æˆ– list of [B, D]

    # Decoder
    precip_pred = self.precip_decoder(precip_output, precip_mask_info)

    # ... è®¡ç®—loss
```

---

## ğŸ”§ å®ç°æ­¥éª¤å»ºè®®

### Phase 1: åŸºç¡€CrossAttentionï¼ˆå¿…é¡»ï¼‰
1. âœ… å®ç°CrossAttentionå’ŒCrossAttentionBlock in `models/layers.py`
2. âœ… ä¿®æ”¹Image Decoderæ”¯æŒCrossAttention
3. âœ… ä¿®æ”¹Vector Decoderæ”¯æŒCrossAttention
4. âœ… æ·»åŠ configé€‰é¡¹ `use_cross_attn = True`
5. âœ… æµ‹è¯•å•ä¸ªmodality (imageæˆ–vector)

### Phase 2: WeightedFeatureMapsï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
1. âœ… å®ç°WeightedFeatureMaps in `models/layers.py`
2. âœ… ä¿®æ”¹Image Encoderè¾“å‡ºå¤šå±‚features
3. âœ… ä¿®æ”¹Vector Encoderè¾“å‡ºå¤šå±‚features
4. âœ… ä¿®æ”¹Decoderæ¥æ”¶å’Œä½¿ç”¨å¤šå±‚features
5. âœ… æ·»åŠ configé€‰é¡¹ `use_weighted_fm = False` (é»˜è®¤å…³é—­)
6. âœ… æµ‹è¯•å¯¹æ¯” with/without WeightedFeatureMaps

### Phase 3: å¯é€‰ä¼˜åŒ–
1. âœ… æ·»åŠ masked self-attentioné€‰é¡¹
2. âœ… æ·»åŠ éƒ¨åˆ†é‡å»ºæœºåˆ¶ (kept_mask_ratio)
3. âœ… Flash Attentionæ”¯æŒ

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### å…¼å®¹æ€§
- âœ… ä¿æŒå‘åå…¼å®¹ï¼š`use_cross_attn=False` æ—¶å›é€€åˆ°æ ‡å‡†MAE
- âœ… åˆ†é˜¶æ®µå®ç°ï¼šå…ˆCrossAttentionï¼Œå†WeightedFeatureMaps
- âœ… å……åˆ†æµ‹è¯•æ¯ä¸ªé˜¶æ®µ

### å†…å­˜ç®¡ç†
- WeightedFeatureMapsä¼šå¢åŠ encoderè¾“å‡ºçš„å­˜å‚¨
- éœ€è¦æƒè¡¡ï¼šæ€§èƒ½æå‡ vs å†…å­˜å ç”¨
- å»ºè®®å…ˆåœ¨å°æ•°æ®é›†æµ‹è¯•

### è¶…å‚æ•°
- `use_fm_layers`: å»ºè®®ç”¨ [0, 3, 5] è€Œä¸æ˜¯æ‰€æœ‰å±‚
- `decoder_self_attn`: é»˜è®¤Falseï¼Œé™¤éæ—¶åºä¾èµ–å¾ˆå¼º

---

## âœ… éªŒè¯æ¸…å•

- [ ] CrossAttention forward/backwardæ­£å¸¸
- [ ] åªç”¨CrossAttentionæ—¶lossä¸‹é™
- [ ] WeightedFeatureMapså¯é€‰å¼€å…³å·¥ä½œ
- [ ] å¼€å¯WeightedFeatureMapsåè®­ç»ƒæ­£å¸¸
- [ ] å†…å­˜å ç”¨åœ¨å¯æ¥å—èŒƒå›´
- [ ] è®­ç»ƒé€Ÿåº¦æå‡ï¼ˆCrossAttention vs Self-Attentionï¼‰
- [ ] å¯¹æ¯”å®éªŒï¼šæ ‡å‡†MAE vs CrossMAE vs CrossMAE+WFM

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### è®¡ç®—æ•ˆç‡
- æ ‡å‡†MAE: 22s/batch
- CrossMAE: é¢„è®¡ 6-8s/batch (3-4xåŠ é€Ÿ)
- CrossMAE+WFM: é¢„è®¡ 7-9s/batch (è½»å¾®å¢åŠ )

### æ€§èƒ½
- CrossMAE: åº”è¯¥ä¸æ ‡å‡†MAEç›¸å½“æˆ–ç•¥å¥½
- CrossMAE+WFM: é¢„è®¡æå‡ 0.1-0.3% (å°å¹…)

---

è¿™ä¸ªè®¡åˆ’æ˜¯å¦æ¸…æ™°ï¼Ÿä½ æƒ³ä»å“ªä¸ªæ­¥éª¤å¼€å§‹å®ç°ï¼Ÿ
