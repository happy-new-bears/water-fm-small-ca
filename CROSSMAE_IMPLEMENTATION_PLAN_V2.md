# âš ï¸ CrossMAEæ”¹é€ è®¡åˆ’ - é‡è¦æ¶æ„å·®å¼‚è¯´æ˜

## ğŸ”´ å…³é”®å‘ç°ï¼šæ¶æ„æ ¹æœ¬å·®å¼‚

### å½“å‰æ¶æ„ (water_fm_small):
```python
Image Encoder:
[B, T, H, W]
  â†’ patchify â†’ [B, T, 522, patch_dim]
  â†’ filter valid â†’ [B, T, 94, patch_dim]
  â†’ remove masked â†’ [B, L_visible_total, patch_dim]  # L_visible_total â‰ˆ 2160
  â†’ transformer â†’ [B, L_visible_total, d_model]
  â†’ **POOLING** â†’ [B, d_model]  â† å…³é”®ï¼špoolæˆå•ä¸ªtoken!

Image Decoder:
encoder_token [B, d_model]
  â†’ åˆ›å»ºå®Œæ•´åºåˆ— [visible + masked]
  â†’ self-attention
  â†’ é¢„æµ‹æ‰€æœ‰ä½ç½®
```

### CrossMAEæ¶æ„:
```python
Image Encoder:
[B, 3, H, W]
  â†’ patchify â†’ [B, L, patch_dim]  # L = 196
  â†’ remove masked â†’ [B, L_visible, patch_dim]  # L_visible â‰ˆ 49
  â†’ transformer â†’ [B, L_visible, d_model]
  â†’ **NO POOLING!** ä¿ç•™åºåˆ— [B, L_visible, d_model]  â† å…³é”®å·®å¼‚!

Image Decoder:
visible_tokens [B, L_visible, d_model]  # ä¿ç•™åºåˆ—ï¼
  â†’ åªåˆ›å»ºmasked queries [B, L_masked, d_model]
  â†’ cross-attention: queries attend to all visible tokens
  â†’ é¢„æµ‹maskedä½ç½®
```

**æ ¸å¿ƒå·®å¼‚ï¼šCrossMAEçš„encoderä¿ç•™äº†åºåˆ—ç»´åº¦ï¼Œæ²¡æœ‰poolæˆå•ä¸ªtokenï¼**

---

## ğŸ¯ ä¸¤ç§å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šè½»é‡çº§æ”¹é€ ï¼ˆå¿«é€Ÿå®ç°ï¼Œä½†ä¸å®Œå…¨ç¬¦åˆCrossMAEï¼‰

**æ”¹åŠ¨**ï¼š
- âœ… Encoderä¿æŒç°æœ‰æ¶æ„ï¼Œpoolæˆ [B, d_model]
- âœ… Decoderç”¨CrossAttentionï¼Œä½†keys/valuesæ˜¯å•ä¸ªtoken
- âŒ æ‰€æœ‰masked positionsçœ‹åˆ°ç›¸åŒçš„context (å¤±å»ç»†ç²’åº¦ä¿¡æ¯)

**ä¼˜ç‚¹**ï¼š
- æ”¹åŠ¨æœ€å°
- å®ç°å¿«é€Ÿ
- èƒ½è·å¾—éƒ¨åˆ†åŠ é€Ÿ

**ç¼ºç‚¹**ï¼š
- ä¸æ˜¯çœŸæ­£çš„CrossMAE
- æ²¡æœ‰å‘æŒ¥CrossMAEçš„æ ¸å¿ƒä¼˜åŠ¿ï¼ˆmasked attend to å¤šä¸ªvisible tokensï¼‰
- æ€§èƒ½æå‡æœ‰é™

---

### æ–¹æ¡ˆBï¼šå®Œæ•´CrossMAEï¼ˆæ¨èï¼Œå½»åº•æ”¹é€ ï¼‰â­

**æ”¹åŠ¨**ï¼š
- ğŸ”„ **é‡å¤§æ”¹åŠ¨**ï¼šEncoderä¸å†poolï¼Œä¿ç•™åºåˆ— [B, L_visible, d_model]
- ğŸ”„ Decoderåˆ›å»ºmasked queriesï¼Œattend toæ‰€æœ‰visible tokens
- ğŸ”„ éœ€è¦ä¿®æ”¹multimodal_mae.pyä¸­çš„fusioné€»è¾‘

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨ç¬¦åˆCrossMAEç²¾ç¥
- âœ… Masked positionså¯ä»¥çœ‹åˆ°æ‰€æœ‰visible positionsçš„ç»†èŠ‚
- âœ… å‘æŒ¥CrossMAEçš„æ ¸å¿ƒä¼˜åŠ¿
- âœ… é¢„æœŸæ€§èƒ½æœ€ä½³

**ç¼ºç‚¹**ï¼š
- æ”¹åŠ¨è¾ƒå¤§ï¼ˆä½†å€¼å¾—ï¼‰
- éœ€è¦é‡æ–°è®¾è®¡modality fusion

---

## ğŸ“‹ é‡‡ç”¨æ–¹æ¡ˆBçš„è¯¦ç»†å®æ–½è®¡åˆ’

### ğŸ”§ Phase 0: æ¶æ„è°ƒæ•´ï¼ˆæ–°å¢ï¼Œæœ€å…³é”®ï¼ï¼‰

#### Step 0.1: ä¿®æ”¹Image Encoder - ç§»é™¤Pooling
**æ–‡ä»¶**: `models/image_encoder.py`

**å½“å‰ä»£ç **:
```python
def forward(self, x_img, patch_mask):
    # ... patchify, filter, position encoding

    # Transformer
    x = self.transformer(x, src_key_padding_mask=padding_mask)

    # âŒ Pooling - è¦ç§»é™¤ï¼
    valid_mask = (~padding_mask).unsqueeze(-1).float()
    encoder_token = (x * valid_mask).sum(dim=1) / valid_mask.sum(dim=1)
    encoder_token = self.norm(encoder_token)  # [B, d_model]

    return encoder_token, mask_info
```

**æ–°ä»£ç ï¼ˆCrossMAEé£æ ¼ï¼‰**:
```python
def forward(self, x_img, patch_mask):
    """
    Returns:
        encoder_output: [B, L_visible, d_model] ä¿ç•™åºåˆ—ï¼
        mask_info: dict
    """
    # ... patchify, filter, position encoding

    # Transformer
    x = self.transformer(x, src_key_padding_mask=padding_mask)

    # âœ… ä¸poolï¼ä¿ç•™åºåˆ—
    # åªåšnormalization
    x = self.norm(x)  # [B, L_visible, d_model]

    mask_info = {
        'mask': patch_mask,
        'padding_mask': padding_mask,  # é‡è¦ï¼šä¼ é€’ç»™decoder
        'positions': positions_padded,
    }

    return x, mask_info  # [B, L_visible, d_model] - ä¿ç•™åºåˆ—ï¼
```

**å…³é”®æ”¹åŠ¨**:
1. âŒ åˆ é™¤poolingæ“ä½œ
2. âœ… è¿”å›å®Œæ•´çš„åºåˆ— [B, L_visible, d_model]
3. âœ… åœ¨mask_infoä¸­ä¼ é€’padding_maskï¼ˆdecoderéœ€è¦ï¼‰

---

#### Step 0.2: ä¿®æ”¹Image Decoder - æ¥æ”¶åºåˆ—
**æ–‡ä»¶**: `models/image_decoder.py`

**å½“å‰ä»£ç **:
```python
def forward(self, encoder_token, mask_info):
    # encoder_token: [B, d_model] å•ä¸ªtoken

    # åˆ›å»ºå®Œæ•´åºåˆ—
    x = self.mask_token.expand(B, T, num_patches, -1)
    x[visible_mask] = encoder_token  # broadcaståˆ°æ‰€æœ‰ä½ç½®

    # Self-attention
    x = self.transformer(x)
    return pred_patches
```

**æ–°ä»£ç ï¼ˆCrossMAEé£æ ¼ï¼‰**:
```python
def forward(self, encoder_output, mask_info):
    """
    Args:
        encoder_output: [B, L_visible, d_model] - åºåˆ—ï¼
        mask_info: dict with 'mask', 'padding_mask', etc.

    Returns:
        pred_patches: [B, T, num_patches, patch_dim]
    """
    mask = mask_info['mask']  # [B, T, num_patches]
    padding_mask = mask_info.get('padding_mask')  # [B, L_visible]

    # Step 1: åˆ›å»ºmasked queriesï¼ˆå‚è€ƒCrossMAE mask_tokens_gridï¼‰
    masked_queries = []
    masked_positions = []  # (b, t, patch_idx)

    for b in range(B):
        for t in range(T):
            for p in range(num_patches):
                if mask[b, t, p]:  # True = masked
                    # Query = mask_token + positional embeddings
                    query = self.mask_token.squeeze() + \
                            self.spatial_pos[0, 0, p] + \
                            self.temporal_pos.pe[0, t]
                    masked_queries.append(query)
                    masked_positions.append((b, t, p))

    if len(masked_queries) == 0:
        return torch.zeros(B, T, num_patches, patch_dim, device=mask.device)

    queries = torch.stack(masked_queries)  # [total_masked, decoder_dim]

    # Step 2: Prepare keys/values from encoder
    # encoder_output: [B, L_visible, encoder_dim]
    # éœ€è¦expandç»™æ¯ä¸ªquery

    # æ–¹å¼1: æŠŠæ‰€æœ‰batchçš„visible tokensæ‹¼åœ¨ä¸€èµ·
    keys_values_list = []
    for b in range(B):
        if padding_mask is not None:
            # åªå–épaddingçš„ä½ç½®
            valid_mask = ~padding_mask[b]  # [L_visible]
            valid_tokens = encoder_output[b, valid_mask]  # [L_valid, encoder_dim]
        else:
            valid_tokens = encoder_output[b]  # [L_visible, encoder_dim]
        keys_values_list.append(valid_tokens)

    # æ–¹å¼2: å¯¹æ¯ä¸ªqueryï¼Œattend to å¯¹åº”batchçš„visible tokens
    # éœ€è¦ä¸ºæ¯ä¸ªqueryè®°å½•å…¶æ‰€å±çš„batch

    # Step 3: CrossAttention decoder blocks
    x = queries.unsqueeze(0)  # [1, total_masked, decoder_dim]

    for blk in self.decoder_blocks:
        # è¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼šæ¯ä¸ªqueryåªattend toè‡ªå·±batchçš„visible tokens
        # æ–¹å¼A: ä½¿ç”¨attention mask
        # æ–¹å¼B: åˆ†batchå¤„ç†
        x = blk(x, keys_values)  # CrossAttentionBlock

    # Step 4: Prediction
    x = self.decoder_norm(x)
    predictions = self.pred_head(x)  # [1, total_masked, patch_dim]

    # Step 5: é‡ç»„
    pred_patches = torch.zeros(B, T, num_patches, patch_dim, device=mask.device)
    for idx, (b, t, p) in enumerate(masked_positions):
        pred_patches[b, t, p] = predictions[0, idx]

    return pred_patches
```

**ä½†æ˜¯ï¼è¿™é‡Œæœ‰ä¸ªé—®é¢˜**ï¼š
- æ¯ä¸ªqueryéœ€è¦attend to**è‡ªå·±batch**çš„visible tokens
- ä¸æ˜¯attend toæ‰€æœ‰batchçš„tokens
- éœ€è¦ç”¨attention maskæˆ–è€…åˆ†batchå¤„ç†

**æ›´å¥½çš„å®ç°ï¼ˆåˆ†batchï¼‰**:
```python
def forward(self, encoder_output, mask_info):
    """
    Args:
        encoder_output: [B, L_visible, d_model]
        mask_info: dict

    Returns:
        pred_patches: [B, T, num_patches, patch_dim]
    """
    mask = mask_info['mask']  # [B, T, num_patches]
    padding_mask = mask_info.get('padding_mask')  # [B, L_visible]
    B, T, num_patches = mask.shape

    pred_patches = torch.zeros(B, T, num_patches, self.patch_dim,
                                device=mask.device, dtype=encoder_output.dtype)

    # é€batchå¤„ç†ï¼ˆæ›´ç®€å•ï¼Œæ›´æ¸…æ™°ï¼‰
    for b in range(B):
        # 1. è·å–è¿™ä¸ªbatchçš„visible tokens
        if padding_mask is not None:
            valid_mask = ~padding_mask[b]
            keys_values = encoder_output[b:b+1, valid_mask]  # [1, L_valid, D]
        else:
            keys_values = encoder_output[b:b+1]  # [1, L_visible, D]

        # 2. åˆ›å»ºè¿™ä¸ªbatchçš„masked queries
        batch_queries = []
        batch_positions = []  # (t, p)

        for t in range(T):
            for p in range(num_patches):
                if mask[b, t, p]:
                    query = self.mask_token.squeeze() + \
                            self.spatial_pos[0, 0, p] + \
                            self.temporal_pos.pe[0, t]
                    batch_queries.append(query)
                    batch_positions.append((t, p))

        if len(batch_queries) == 0:
            continue

        queries = torch.stack(batch_queries).unsqueeze(0)  # [1, L_masked_b, D]

        # 3. CrossAttention decoder
        x = queries
        for blk in self.decoder_blocks:
            x = blk(x, keys_values)  # queries attend to this batch's visible

        # 4. Prediction
        x = self.decoder_norm(x)
        predictions = self.pred_head(x)  # [1, L_masked_b, patch_dim]

        # 5. Fill predictions
        for idx, (t, p) in enumerate(batch_positions):
            pred_patches[b, t, p] = predictions[0, idx]

    return pred_patches
```

---

### ğŸ”§ Phase 1: CrossAttentionæ¨¡å—å®ç°

ï¼ˆä¿æŒä¸å˜ï¼Œå‚è€ƒåŸè®¡åˆ’ï¼‰

---

### ğŸ”§ Phase 2: WeightedFeatureMapsï¼ˆå¯é€‰ï¼‰

#### å…³é”®æ”¹åŠ¨ï¼šä¿å­˜å¤šå±‚çš„**åºåˆ—è¾“å‡º**

**Image Encoder with WeightedFeatureMaps**:
```python
def forward(self, x_img, patch_mask):
    # ... patchify, filter, position encoding

    if self.use_weighted_fm:
        x_feats = []

        if self.use_input:
            x_feats.append(x.clone())  # [B, L_visible, d_model]

        for idx, blk in enumerate(self.transformer.layers):
            x = blk(x, src_key_padding_mask=padding_mask)

            if idx in self.use_fm_layers:
                x_feats.append(self.norm(x.clone()))  # ä¿å­˜åºåˆ—ï¼

        # è¿”å›list of [B, L_visible, d_model]
        return x_feats, mask_info
    else:
        x = self.transformer(x, src_key_padding_mask=padding_mask)
        x = self.norm(x)
        return x, mask_info  # [B, L_visible, d_model]
```

**Image Decoder with WeightedFeatureMaps**:
```python
def forward(self, encoder_output, mask_info):
    # encoder_output: list of [B, L_visible, d_model]

    # å¯¹æ¯ä¸ªbatchçš„æ¯ä¸ªmasked position:
    for b in range(B):
        if padding_mask is not None:
            valid_mask = ~padding_mask[b]

        # Keys/Values: æ¥è‡ªå¤šå±‚encoder
        if self.use_weighted_fm:
            # æå–è¿™ä¸ªbatchçš„æ‰€æœ‰å±‚çš„visible tokens
            kv_layers = []
            for feat in encoder_output:  # list
                if padding_mask is not None:
                    kv_layers.append(feat[b:b+1, valid_mask])  # [1, L_valid, D]
                else:
                    kv_layers.append(feat[b:b+1])

            # WeightedFeatureMaps: ç»„åˆå¤šå±‚
            # éœ€è¦åœ¨ [1, L_valid, D, num_layers] ä¸Šæ“ä½œ
            stacked_kv = torch.stack(kv_layers, dim=-1)  # [1, L_valid, D, k]
            weighted_kv = self.wfm(stacked_kv)  # [1, L_valid, D, decoder_depth]
        else:
            keys_values = encoder_output[b:b+1, valid_mask]

        # ... åˆ›å»ºqueries

        # CrossAttention with weighted features
        x = queries
        for i, blk in enumerate(self.decoder_blocks):
            if self.use_weighted_fm:
                # ç¬¬iä¸ªdecoderå±‚ç”¨ç¬¬iä¸ªfeatureç»„åˆ
                kv_i = self.dec_norms[i](weighted_kv[..., i])  # [1, L_valid, D]
                x = blk(x, kv_i)
            else:
                x = blk(x, keys_values)
```

---

## ğŸš¨ éœ€è¦é¢å¤–å¤„ç†çš„é—®é¢˜

### é—®é¢˜1: Modality Fusion
**å½“å‰**: æ¯ä¸ªmodalityçš„encoderè¾“å‡º [B, d_model]ï¼Œå¯ä»¥ç›´æ¥concatæˆ–add
```python
fused = torch.cat([precip_token, soil_token, temp_token, ...], dim=-1)
```

**CrossMAEé£æ ¼**: æ¯ä¸ªmodalityè¾“å‡º [B, L_visible, d_model]
- L_visibleæ¯ä¸ªmodalityéƒ½ä¸åŒï¼
- å¦‚ä½•fusionï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. **Option A**: æ¯ä¸ªmodalityåˆ†åˆ«decoderï¼Œä¸åšfusion
2. **Option B**: Fusionæ—¶å…ˆpoolå„modalityï¼Œç„¶åç”¨fused tokenä½œä¸ºé¢å¤–çš„keys/values
3. **Option C**: åœ¨decoderä¸­åšcross-modality attention

æ¨èï¼š**Option A** (ç®€å•) æˆ– **Option B** (ä¿ç•™fusionèƒ½åŠ›)

---

### é—®é¢˜2: Static Attributes
**å½“å‰**: Static attributesä¸encoder token concat
```python
encoder_token = torch.cat([encoder_token, static_attr], dim=-1)
```

**CrossMAEé£æ ¼**: Encoderè¾“å‡ºæ˜¯åºåˆ— [B, L_visible, d_model]

**è§£å†³æ–¹æ¡ˆ**:
```python
# Option A: Static attråŠ åˆ°æ¯ä¸ªvisible tokenä¸Š
static_expanded = static_attr.unsqueeze(1).expand(-1, L_visible, -1)
encoder_output = torch.cat([encoder_output, static_expanded], dim=-1)

# Option B: Static atträ½œä¸ºé¢å¤–çš„token
static_token = self.static_proj(static_attr).unsqueeze(1)  # [B, 1, d_model]
encoder_output = torch.cat([encoder_output, static_token], dim=1)  # [B, L_visible+1, d_model]
```

æ¨èï¼š**Option B** (æ›´ç¬¦åˆtransformeré£æ ¼)

---

## âœ… æœ€ç»ˆå®æ–½æ­¥éª¤

### Phase 0: æ¶æ„è°ƒæ•´ï¼ˆæ–°å¢ï¼‰â­
1. [x] ä¿®æ”¹Image Encoderç§»é™¤pooling
2. [x] ä¿®æ”¹Image Decoderæ¥æ”¶åºåˆ—å¹¶å®ç°é€batchå¤„ç†
3. [x] ä¿®æ”¹Vector Encoderç§»é™¤pooling
4. [x] ä¿®æ”¹Vector Decoderæ¥æ”¶åºåˆ—
5. [x] è°ƒæ•´Static Attributeså¤„ç†
6. [x] æµ‹è¯•åŸºç¡€åŠŸèƒ½ï¼ˆä¸ç”¨CrossAttentionï¼Œåªæ˜¯æ¶æ„æ”¹å˜ï¼‰

### Phase 1: CrossAttention
7. [x] å®ç°CrossAttentionå’ŒCrossAttentionBlock
8. [x] æ›¿æ¢Decoderä¸­çš„self-attentionä¸ºcross-attention
9. [x] æµ‹è¯•CrossAttentionç‰ˆæœ¬

### Phase 2: WeightedFeatureMapsï¼ˆå¯é€‰ï¼‰
10. [x] å®ç°WeightedFeatureMaps
11. [x] ä¿®æ”¹Encoderä¿å­˜å¤šå±‚åºåˆ—
12. [x] ä¿®æ”¹Decoderä½¿ç”¨å¤šå±‚features
13. [x] å¯¹æ¯”æµ‹è¯•

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### Phase 0å®Œæˆå:
- æ¶æ„ç¬¦åˆCrossMAE
- ä½†ä»ç”¨self-attention
- æ€§èƒ½ä¸åŸç‰ˆç›¸å½“

### Phase 1å®Œæˆå:
- å®Œæ•´CrossMAE
- é¢„è®¡åŠ é€Ÿ 3-4å€
- æ€§èƒ½ç›¸å½“æˆ–ç•¥å¥½

### Phase 2å®Œæˆå:
- CrossMAE + WeightedFeatureMaps
- é¢„è®¡é¢å¤–æå‡ 0.1-0.3%
- å†…å­˜å¢åŠ é€‚ä¸­

---

è¿™ä¸ªæ›´æ–°åçš„è®¡åˆ’æ˜¯å¦æ¸…æ™°ï¼Ÿä¸»è¦æ–°å¢äº†**Phase 0ï¼ˆæ¶æ„è°ƒæ•´ï¼‰**ï¼Œè¿™æ˜¯æœ€å…³é”®çš„æ”¹åŠ¨ã€‚
